{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rabah\\Documents\\evry\\M2 CNS\\masteriale\\SimpleLLM\\Venv\\Lib\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from mamba_model import ModelArgs, init_model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-12-21T17:56:07.680090900Z",
     "start_time": "2025-12-21T17:55:59.027805200Z"
    }
   },
   "id": "63847c4962e860ad",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def load_mamba_model(model_dir=\"mamba_model_sauvegarde\"):\n",
    "    # Charger la configuration\n",
    "    with open(os.path.join(model_dir, \"config.json\"), \"r\") as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    # Charger le tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "    # Reconstruire le modèle\n",
    "    args = ModelArgs(\n",
    "        model_input_dims=config['model_input_dims'],\n",
    "        model_states=config['model_states'],\n",
    "        num_layers=config['num_layers'],\n",
    "        dropout_rate=config['dropout_rate'],\n",
    "        vocab_size=config['vocab_size'],\n",
    "        num_classes=config['num_classes'],\n",
    "        seq_length=config['seq_length'],\n",
    "        conv_kernel_size=config['conv_kernel_size'],\n",
    "        use_lm_head=config['use_lm_head'],\n",
    "        loss=config['loss'],\n",
    "    )\n",
    "\n",
    "    model = init_model(args)\n",
    "\n",
    "    # Charger les poids\n",
    "    model.load_weights(os.path.join(model_dir, \"model_weights.weights.h5\"))\n",
    "\n",
    "    return model, config, tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-12-21T17:56:09.708928Z",
     "start_time": "2025-12-21T17:56:09.702415100Z"
    }
   },
   "id": "dd8fdc349b37fc7e",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate_text(prompt, model, config, tokenizer, max_new_tokens=100,temperature=0.7, top_p=0.9, top_k=50):\n",
    "    seq_length = config['seq_length']\n",
    "\n",
    "    # Tokeniser le prompt\n",
    "    prompt_tokens = tokenizer.encode(prompt)\n",
    "\n",
    "    # Créer une séquence avec padding\n",
    "    input_ids = np.array([prompt_tokens], dtype=np.int32)\n",
    "\n",
    "    # Compléter avec du padding si nécessaire\n",
    "    if input_ids.shape[1] < seq_length:\n",
    "        padding = np.full((1, seq_length - input_ids.shape[1]), tokenizer.pad_token_id, dtype=np.int32)\n",
    "        input_ids = np.concatenate([input_ids, padding], axis=1)\n",
    "    else:\n",
    "        input_ids = input_ids[:, :seq_length]\n",
    "\n",
    "    generated_tokens = prompt_tokens.copy()\n",
    "\n",
    "    for step in range(max_new_tokens):\n",
    "        logits = model.predict(input_ids, verbose=0)\n",
    "\n",
    "        next_logits = logits[0, -1, :]\n",
    "\n",
    "        # Appliquer la temperature\n",
    "        next_logits = next_logits / temperature\n",
    "\n",
    "        # Softmax pour obtenir les probabilités\n",
    "        probs = tf.nn.softmax(next_logits).numpy()\n",
    "\n",
    "        # Top-k filtering\n",
    "        if top_k > 0:\n",
    "            top_k_indices = np.argsort(probs)[-top_k:]\n",
    "            top_k_probs = probs[top_k_indices]\n",
    "            top_k_probs = top_k_probs / top_k_probs.sum()\n",
    "\n",
    "            next_token = np.random.choice(top_k_indices, p=top_k_probs)\n",
    "        else:\n",
    "            # Top-p (nucleus) sampling\n",
    "            sorted_indices = np.argsort(probs)[::-1]\n",
    "            sorted_probs = probs[sorted_indices]\n",
    "            cumsum_probs = np.cumsum(sorted_probs)\n",
    "\n",
    "            # Garder seulement les tokens dans le top-p\n",
    "            valid_mask = cumsum_probs <= top_p\n",
    "            if not np.any(valid_mask):\n",
    "                valid_mask[0] = True\n",
    "\n",
    "            valid_indices = sorted_indices[valid_mask]\n",
    "            valid_probs = probs[valid_indices]\n",
    "            valid_probs = valid_probs / valid_probs.sum()\n",
    "\n",
    "            next_token = np.random.choice(valid_indices, p=valid_probs)\n",
    "\n",
    "        generated_tokens.append(next_token)\n",
    "\n",
    "        # Vérifier si on a généré un token de fin\n",
    "        if next_token in [tokenizer.pad_token_id, tokenizer.sep_token_id, tokenizer.eos_token_id]:\n",
    "            break\n",
    "\n",
    "        # Mettre à jour input_ids (fenêtre glissante)\n",
    "        next_input = np.array([[next_token]], dtype=np.int32)\n",
    "        input_ids = np.concatenate([input_ids[:, 1:], next_input], axis=1)\n",
    "\n",
    "        if (step + 1) % 20 == 0:\n",
    "            print(f\"  {step + 1}/{max_new_tokens} tokens générés...\")\n",
    "\n",
    "    # token vers texte\n",
    "    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "    return generated_text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-12-21T17:56:20.553985100Z",
     "start_time": "2025-12-21T17:56:20.545463700Z"
    }
   },
   "id": "559b2b62d934d99",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rabah\\Documents\\evry\\M2 CNS\\masteriale\\SimpleLLM\\Venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:100: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rabah\\Documents\\evry\\M2 CNS\\masteriale\\SimpleLLM\\Venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:106: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rabah\\Documents\\evry\\M2 CNS\\masteriale\\SimpleLLM\\Venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rabah\\Documents\\evry\\M2 CNS\\masteriale\\SimpleLLM\\Venv\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:797: UserWarning: Skipping variable loading for optimizer 'adamw', because it has 2 variables whereas the saved optimizer has 232 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "model, config, tokenizer = load_mamba_model(\"mamba_model_sauvegarde\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-12-21T17:56:25.502133100Z",
     "start_time": "2025-12-21T17:56:21.949220800Z"
    }
   },
   "id": "89aa00fc9465c95",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  20/100 tokens générés...\n",
      "  40/100 tokens générés...\n",
      "  60/100 tokens générés...\n",
      "  80/100 tokens générés...\n",
      "  100/100 tokens générés...\n",
      "machine learning is set work in @ on \"walk have of, on girl and un was \" the before ier single of as into a 1965 record while that s performance @ doubt music a 1965 at she allmusic in of news who from was at set no felt 1965 byju. had by @ju a of were sun stage ] boys of - that ' the into itsno on was song ] by themselves images s only what as \" for. us next album ross also london s a allmusic on \"eti left of\n"
     ]
    }
   ],
   "source": [
    "texte = generate_text(\n",
    "    \"Machine learning is\",\n",
    "    model, config, tokenizer,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.8\n",
    ")\n",
    "print(texte)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-12-21T17:58:09.870648300Z",
     "start_time": "2025-12-21T17:57:45.904252600Z"
    }
   },
   "id": "22a3ba9b64fb07f4",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "84724da155a7dfcd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
